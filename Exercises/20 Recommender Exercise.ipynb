{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt\n",
    "Build a recommendation system using all or some of movie info\n",
    "Get only dataset link from notebook\n",
    "\n",
    "#### Hints\n",
    " - Combine movie data into one string since TFidf only takes one string as a individual document\n",
    " - Use TF-IDF to transform strings into vectors. \n",
    " - Get the TF-IDF of a query movie, compute similarity between query and other vectors\n",
    " - Sort by similarity then return the top 5 closest movies\n",
    " - Test on movies in other genres to test if code works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\seohy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seohy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\seohy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download & check raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-10-07 18:58:53--  https://lazyprogrammer.me/course_files/nlp/tmdb_5000_movies.csv\n",
      "Resolving lazyprogrammer.me (lazyprogrammer.me)... 172.67.213.166, 104.21.23.210\n",
      "Connecting to lazyprogrammer.me (lazyprogrammer.me)|172.67.213.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5698602 (5.4M) [text/csv]\n",
      "Saving to: 'tmdb_5000_movies.csv.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 1.50M 4s\n",
      "    50K .......... .......... .......... .......... ..........  1% 6.34M 2s\n",
      "   100K .......... .......... .......... .......... ..........  2% 2.32M 2s\n",
      "   150K .......... .......... .......... .......... ..........  3% 4.83M 2s\n",
      "   200K .......... .......... .......... .......... ..........  4% 4.79M 2s\n",
      "   250K .......... .......... .......... .......... ..........  5% 2.83M 2s\n",
      "   300K .......... .......... .......... .......... ..........  6% 8.06M 2s\n",
      "   350K .......... .......... .......... .......... ..........  7% 1.20M 2s\n",
      "   400K .......... .......... .......... .......... ..........  8% 2.77M 2s\n",
      "   450K .......... .......... .......... .......... ..........  8% 59.1M 2s\n",
      "   500K .......... .......... .......... .......... ..........  9% 12.3M 2s\n",
      "   550K .......... .......... .......... .......... .......... 10% 39.2M 1s\n",
      "   600K .......... .......... .......... .......... .......... 11% 38.9M 1s\n",
      "   650K .......... .......... .......... .......... .......... 12% 10.8M 1s\n",
      "   700K .......... .......... .......... .......... .......... 13% 50.2M 1s\n",
      "   750K .......... .......... .......... .......... .......... 14% 2.50M 1s\n",
      "   800K .......... .......... .......... .......... .......... 15% 4.01M 1s\n",
      "   850K .......... .......... .......... .......... .......... 16% 13.2M 1s\n",
      "   900K .......... .......... .......... .......... .......... 17% 21.8M 1s\n",
      "   950K .......... .......... .......... .......... .......... 17% 2.18M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 18% 14.3M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 19% 25.7M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 20% 4.74M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 21% 12.6M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 22% 1.38M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 23% 2.24M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 24% 35.7M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 25% 58.5M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 26% 52.7M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 26% 62.9M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 27% 67.5M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 28% 25.1M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 29% 13.3M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 30% 37.8M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 31% 50.6M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 32% 5.25M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 33% 43.9M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 34% 62.9M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 35% 40.3M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 35% 16.2M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 36% 61.4M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 37% 11.4M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 38% 33.5M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 39% 56.6M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 40% 14.7M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 41% 62.6M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 42% 4.81M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 43% 18.7M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 44% 35.8M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 44% 39.3M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 45% 45.2M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 46% 13.0M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 47% 34.2M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 48% 12.4M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 49% 4.92M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 50% 3.20M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 51% 17.3M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 52% 2.42M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 53% 33.5M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 53% 11.2M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 54% 30.9M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 55% 7.84M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 56% 14.4M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 57% 22.2M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 58% 31.7M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 59% 4.49M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 60% 30.3M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 61% 43.8M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 61% 34.1M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 62% 44.9M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 63% 41.6M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 64% 15.9M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 65% 8.15M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 66% 33.3M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 67% 51.5M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 68% 12.3M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 69% 34.6M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 70% 2.09M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 70% 24.6M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 71% 26.4M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 72% 43.5M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 73% 55.8M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 74% 19.7M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 75% 21.5M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 76% 7.59M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 77% 39.2M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 78% 11.0M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 79% 34.9M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 79% 12.2M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 80% 8.70M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 81% 3.98M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 82% 28.7M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 83% 20.5M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 84% 6.29M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 85% 33.3M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 86% 14.0M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 87% 2.98M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 88% 52.5M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 88% 48.2M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 89% 5.82M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 90% 2.62M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 91% 51.4M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 92% 13.8M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 93% 16.9M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 94% 42.9M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 95% 5.71M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 96% 49.7M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 97% 16.6M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 97% 4.73M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 98% 38.0M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 99% 36.8M 0s\n",
      "  5550K .......... .....                                      100% 40.8M=0.6s\n",
      "\n",
      "2023-10-07 18:58:54 (8.40 MB/s) - 'tmdb_5000_movies.csv.1' saved [5698602/5698602]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/tmdb/tmdb-movie-metadata\n",
    "!wget https://lazyprogrammer.me/course_files/nlp/tmdb_5000_movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tmdb_5000_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...</td>\n",
       "      <td>http://www.avatarmovie.com/</td>\n",
       "      <td>19995</td>\n",
       "      <td>[{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...</td>\n",
       "      <td>en</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>150.437577</td>\n",
       "      <td>[{\"name\": \"Ingenious Film Partners\", \"id\": 289...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>2787965087</td>\n",
       "      <td>162.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Enter the World of Pandora.</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300000000</td>\n",
       "      <td>[{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...</td>\n",
       "      <td>http://disney.go.com/disneypictures/pirates/</td>\n",
       "      <td>285</td>\n",
       "      <td>[{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...</td>\n",
       "      <td>en</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Captain Barbossa, long believed to be dead, ha...</td>\n",
       "      <td>139.082615</td>\n",
       "      <td>[{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2007-05-19</td>\n",
       "      <td>961000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>At the end of the world, the adventure begins.</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      budget                                             genres  \\\n",
       "0  237000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
       "1  300000000  [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...   \n",
       "\n",
       "                                       homepage     id  \\\n",
       "0                   http://www.avatarmovie.com/  19995   \n",
       "1  http://disney.go.com/disneypictures/pirates/    285   \n",
       "\n",
       "                                            keywords original_language  \\\n",
       "0  [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...                en   \n",
       "1  [{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...                en   \n",
       "\n",
       "                             original_title  \\\n",
       "0                                    Avatar   \n",
       "1  Pirates of the Caribbean: At World's End   \n",
       "\n",
       "                                            overview  popularity  \\\n",
       "0  In the 22nd century, a paraplegic Marine is di...  150.437577   \n",
       "1  Captain Barbossa, long believed to be dead, ha...  139.082615   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  [{\"name\": \"Ingenious Film Partners\", \"id\": 289...   \n",
       "1  [{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...   \n",
       "\n",
       "                                production_countries release_date     revenue  \\\n",
       "0  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2009-12-10  2787965087   \n",
       "1  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2007-05-19   961000000   \n",
       "\n",
       "   runtime                                   spoken_languages    status  \\\n",
       "0    162.0  [{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...  Released   \n",
       "1    169.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]  Released   \n",
       "\n",
       "                                          tagline  \\\n",
       "0                     Enter the World of Pandora.   \n",
       "1  At the end of the world, the adventure begins.   \n",
       "\n",
       "                                      title  vote_average  vote_count  \n",
       "0                                    Avatar           7.2       11800  \n",
       "1  Pirates of the Caribbean: At World's End           6.9        4500  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizing DF to pass into TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Turning meaningful columns for recommendation system (genre, keywords, overview, tagline) into a single single string to be passed into TF-IDF. \n",
    "\n",
    "Step-by-step plan:\n",
    "1. Extract genre names from 'genres' column, store into a variable. \n",
    "2. For each keyword from 'keywords' column, concat them together, store into a variable\n",
    "3. Store overview from 'overview column' into a varible\n",
    "4. Store tagline from 'tagline' into a variable\n",
    "5. Add variable in steps 1~5 into one variable\n",
    "6. Add the resultant string into a list.\n",
    "7. The resultant list is a list of all train + test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Action  Adventure  Fantasy  Science Fiction  ...\n",
       "1                      Adventure  Fantasy  Action        \n",
       "2                        Action  Adventure  Crime        \n",
       "3                    Action  Crime  Drama  Thriller      \n",
       "4              Action  Adventure  Science Fiction        \n",
       "                              ...                        \n",
       "4798                      Action  Crime  Thriller        \n",
       "4799                            Comedy  Romance          \n",
       "4800               Comedy  Drama  Romance  TV Movie      \n",
       "4801                                                     \n",
       "4802                              Documentary            \n",
       "Length: 4803, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_genres = df['genres'].str[1:-1]\n",
    "split_genres = split_genres.str.split(', ', expand=True)\n",
    "\n",
    "# Extract genre using regex\n",
    "# Iterating over each row is slow and inefficient. Use applymap or apply instead.\n",
    "def extract_name(cell):\n",
    "    if pd.isnull(cell):\n",
    "        return ''\n",
    "    match = re.search(r'\"name\":\\s*\"([^\"]*)\"', cell) # cannot search through None\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Apply extract_genre function\n",
    "genre_extracted_df = split_genres.applymap(extract_name)\n",
    "genre_extracted_df = genre_extracted_df.fillna('')\n",
    "\n",
    "genre_extracted_df = genre_extracted_df.agg(' '. join, axis=1)\n",
    "\n",
    "genre_extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        culture clash  future  space war  space colon...\n",
       "1        ocean  drug abuse  exotic island  east india ...\n",
       "2        spy  based on novel  secret agent  sequel  mi...\n",
       "3        dc comics  crime fighter  terrorist  secret i...\n",
       "4        based on novel  mars  medallion  space travel...\n",
       "                              ...                        \n",
       "4798     united states\\u2013mexico barrier  legs  arms...\n",
       "4799                                                  ...\n",
       "4800     date  love at first sight  narration  investi...\n",
       "4801                                                  ...\n",
       "4802     obsession  camcorder  crush  dream girl      ...\n",
       "Length: 4803, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_keywords = df['keywords'].str[1:-1]\n",
    "split_keywords = split_keywords.str.split(', ', expand=True)\n",
    "\n",
    "keywords_extracted_df = split_keywords.applymap(extract_name)\n",
    "keywords_extracted_df = keywords_extracted_df.fillna('')\n",
    "\n",
    "keywords_extracted_df = keywords_extracted_df.agg(' '. join, axis=1)\n",
    "\n",
    "keywords_extracted_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_df = df['overview']\n",
    "tagline_df = df['tagline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Action  Adventure  Fantasy  Science Fiction  ...\n",
       "1        Adventure  Fantasy  Action          ocean  dr...\n",
       "2        Action  Adventure  Crime          spy  based ...\n",
       "3        Action  Crime  Drama  Thriller        dc comi...\n",
       "4        Action  Adventure  Science Fiction          b...\n",
       "                              ...                        \n",
       "4798     Action  Crime  Thriller          united state...\n",
       "4799     Comedy  Romance                              ...\n",
       "4800     Comedy  Drama  Romance  TV Movie        date ...\n",
       "4801                                                  ...\n",
       "4802     Documentary              obsession  camcorder...\n",
       "Length: 4803, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = pd.concat([genre_extracted_df, keywords_extracted_df, overview_df, tagline_df], axis=1)\n",
    "total_df = total_df.fillna('')\n",
    "\n",
    "total_df = total_df.agg(' '. join, axis=1)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using TF-IDF\n",
    "\n",
    "No need to split the data into train and test set, because there is no testing to be done. \n",
    "Rather, we are calculating the vector distance between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = total_df \n",
    "\n",
    "tfidf = TfidfVectorizer() # instantiate TfidfVectorizer class\n",
    "                          # try out other 변수 like stopwords\n",
    "tfidf_matrix = tfidf.fit_transform(train_texts) # fit vectorizer onto data, transform into vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get query, calculate the closest 5 vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TF-IDF of a query movie, compute similarity between query and other vectors\n",
    "query_movie = input('Which movie do you wish to watch: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your query, I would recommended: The Avengers, Semi-Pro, R.I.P.D., Quantum of Solace, Avatar\n"
     ]
    }
   ],
   "source": [
    "# take query_movie find corresponding vector in df\n",
    "query_movie_index = df[df['original_title'] == query_movie].index[0]\n",
    "query_vector = tfidf_matrix[query_movie_index]\n",
    "\n",
    "# calculate cosine distance between corresponding vector and all other vectors using df\n",
    "cosine_similarity_list = []\n",
    "for i in range(1, 4803):\n",
    "    query_vector_1D = query_vector.toarray().flatten()\n",
    "    tfidf_matrix_1D = tfidf_matrix[i].toarray().flatten()\n",
    "    cosine_similarity = 1 - spatial.distance.cosine(query_vector_1D, tfidf_matrix_1D)\n",
    "    cosine_similarity_list.append(cosine_similarity)\n",
    "\n",
    "# rank the distance, select the 5 closest vectors\n",
    "index = np.argpartition(cosine_similarity_list, -5)[-5:]\n",
    "\n",
    "# return the movies that correspond with those 5 closest vectors using df\n",
    "print('Based on your query, I would recommended: {0}, {1}, {2}, {3}, {4}'.format(\n",
    "    df.loc[index[0],'original_title'], \n",
    "    df.loc[index[1],'original_title'], \n",
    "    df.loc[index[2],'original_title'], \n",
    "    df.loc[index[3],'original_title'], \n",
    "    df.loc[index[4],'original_title']\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
