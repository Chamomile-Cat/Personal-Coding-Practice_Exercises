{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt\n",
    "Build a recommendation system using all or some of movie info\n",
    "Get only dataset link from notebook\n",
    "\n",
    "#### Hints\n",
    " - Combine movie data into one string since TFidf only takes one string as a individual document\n",
    " - Use TF-IDF to transform strings into vectors. \n",
    " - Get the TF-IDF of a query movie, compute similarity between query and other vectors\n",
    " - Sort by similarity then return the top 5 closest movies\n",
    " - Test on movies in other genres to test if code works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import spatial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\seohy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seohy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\seohy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download & check raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-10-07 19:00:21--  https://lazyprogrammer.me/course_files/nlp/tmdb_5000_movies.csv\n",
      "Resolving lazyprogrammer.me (lazyprogrammer.me)... 172.67.213.166, 104.21.23.210\n",
      "Connecting to lazyprogrammer.me (lazyprogrammer.me)|172.67.213.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5698602 (5.4M) [text/csv]\n",
      "Saving to: 'tmdb_5000_movies.csv.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 1.29M 4s\n",
      "    50K .......... .......... .......... .......... ..........  1% 2.38M 3s\n",
      "   100K .......... .......... .......... .......... ..........  2% 1.72M 3s\n",
      "   150K .......... .......... .......... .......... ..........  3% 2.80M 3s\n",
      "   200K .......... .......... .......... .......... ..........  4% 2.96M 3s\n",
      "   250K .......... .......... .......... .......... ..........  5% 3.57M 2s\n",
      "   300K .......... .......... .......... .......... ..........  6% 7.31M 2s\n",
      "   350K .......... .......... .......... .......... ..........  7% 6.07M 2s\n",
      "   400K .......... .......... .......... .......... ..........  8% 6.46M 2s\n",
      "   450K .......... .......... .......... .......... ..........  8% 5.97M 2s\n",
      "   500K .......... .......... .......... .......... ..........  9% 8.86M 2s\n",
      "   550K .......... .......... .......... .......... .......... 10% 8.85M 1s\n",
      "   600K .......... .......... .......... .......... .......... 11% 6.14M 1s\n",
      "   650K .......... .......... .......... .......... .......... 12% 12.6M 1s\n",
      "   700K .......... .......... .......... .......... .......... 13% 5.66M 1s\n",
      "   750K .......... .......... .......... .......... .......... 14% 24.4M 1s\n",
      "   800K .......... .......... .......... .......... .......... 15% 15.1M 1s\n",
      "   850K .......... .......... .......... .......... .......... 16% 21.3M 1s\n",
      "   900K .......... .......... .......... .......... .......... 17% 18.5M 1s\n",
      "   950K .......... .......... .......... .......... .......... 17% 13.5M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 18% 14.0M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 19% 8.21M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 20% 5.63M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 21% 1.82M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 22% 43.1M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 23% 5.34M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 24% 22.9M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 25% 76.8M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 26% 4.32M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 26% 36.0M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 27% 35.0M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 28% 16.7M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 29% 85.2M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 30% 83.2M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 31% 4.55M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 32% 23.0M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 33% 34.7M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 34% 5.68M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 35% 12.9M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 35% 42.1M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 36% 57.4M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 37% 27.0M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 38% 38.2M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 39% 85.7M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 40% 89.7M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 41% 48.8M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 42% 17.0M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 43% 12.4M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 44% 9.99M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 44% 35.6M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 45% 52.2M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 46% 3.48M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 47% 33.5M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 48% 45.0M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 49% 16.2M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 50% 9.89M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 51% 8.75M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 52% 53.2M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 53% 9.22M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 53% 33.6M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 54% 37.2M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 55% 50.4M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 56% 16.1M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 57% 9.68M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 58% 39.7M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 59% 4.60M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 60% 27.2M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 61% 58.7M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 61% 73.3M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 62% 46.2M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 63% 4.15M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 64% 6.99M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 65% 23.9M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 66% 52.8M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 67% 64.4M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 68% 82.6M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 69% 61.0M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 70% 64.4M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 70% 73.3M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 71% 60.3M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 72% 20.4M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 73% 28.0M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 74% 20.0M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 75% 2.40M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 76% 54.6M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 77% 10.2M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 78% 33.7M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 79% 15.4M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 79% 75.6M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 80% 60.9M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 81% 61.8M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 82% 7.70M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 83% 13.3M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 84% 62.3M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 85% 3.63M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 86% 29.3M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 87% 67.8M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 88% 76.7M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 88% 6.85M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 89% 42.1M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 90% 41.3M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 91% 5.41M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 92% 4.39M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 93% 6.45M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 94% 10.5M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 95% 40.9M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 96% 16.7M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 97% 58.8M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 97% 48.6M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 98% 56.4M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 99% 58.9M 0s\n",
      "  5550K .......... .....                                      100% 92.6M=0.5s\n",
      "\n",
      "2023-10-07 19:00:22 (10.0 MB/s) - 'tmdb_5000_movies.csv.1' saved [5698602/5698602]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/tmdb/tmdb-movie-metadata\n",
    "!wget https://lazyprogrammer.me/course_files/nlp/tmdb_5000_movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tmdb_5000_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...</td>\n",
       "      <td>http://www.avatarmovie.com/</td>\n",
       "      <td>19995</td>\n",
       "      <td>[{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...</td>\n",
       "      <td>en</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>150.437577</td>\n",
       "      <td>[{\"name\": \"Ingenious Film Partners\", \"id\": 289...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>2787965087</td>\n",
       "      <td>162.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Enter the World of Pandora.</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300000000</td>\n",
       "      <td>[{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...</td>\n",
       "      <td>http://disney.go.com/disneypictures/pirates/</td>\n",
       "      <td>285</td>\n",
       "      <td>[{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...</td>\n",
       "      <td>en</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Captain Barbossa, long believed to be dead, ha...</td>\n",
       "      <td>139.082615</td>\n",
       "      <td>[{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...</td>\n",
       "      <td>[{\"iso_3166_1\": \"US\", \"name\": \"United States o...</td>\n",
       "      <td>2007-05-19</td>\n",
       "      <td>961000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>At the end of the world, the adventure begins.</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      budget                                             genres  \\\n",
       "0  237000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
       "1  300000000  [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"...   \n",
       "\n",
       "                                       homepage     id  \\\n",
       "0                   http://www.avatarmovie.com/  19995   \n",
       "1  http://disney.go.com/disneypictures/pirates/    285   \n",
       "\n",
       "                                            keywords original_language  \\\n",
       "0  [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...                en   \n",
       "1  [{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na...                en   \n",
       "\n",
       "                             original_title  \\\n",
       "0                                    Avatar   \n",
       "1  Pirates of the Caribbean: At World's End   \n",
       "\n",
       "                                            overview  popularity  \\\n",
       "0  In the 22nd century, a paraplegic Marine is di...  150.437577   \n",
       "1  Captain Barbossa, long believed to be dead, ha...  139.082615   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  [{\"name\": \"Ingenious Film Partners\", \"id\": 289...   \n",
       "1  [{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...   \n",
       "\n",
       "                                production_countries release_date     revenue  \\\n",
       "0  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2009-12-10  2787965087   \n",
       "1  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   2007-05-19   961000000   \n",
       "\n",
       "   runtime                                   spoken_languages    status  \\\n",
       "0    162.0  [{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...  Released   \n",
       "1    169.0           [{\"iso_639_1\": \"en\", \"name\": \"English\"}]  Released   \n",
       "\n",
       "                                          tagline  \\\n",
       "0                     Enter the World of Pandora.   \n",
       "1  At the end of the world, the adventure begins.   \n",
       "\n",
       "                                      title  vote_average  vote_count  \n",
       "0                                    Avatar           7.2       11800  \n",
       "1  Pirates of the Caribbean: At World's End           6.9        4500  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizing DF to pass into TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Turning meaningful columns for recommendation system (genre, keywords, overview, tagline) into a single single string to be passed into TF-IDF. \n",
    "\n",
    "Step-by-step plan:\n",
    "1. Extract genre names from 'genres' column, store into a variable. \n",
    "2. For each keyword from 'keywords' column, concat them together, store into a variable\n",
    "3. Store overview from 'overview column' into a varible\n",
    "4. Store tagline from 'tagline' into a variable\n",
    "5. Add variable in steps 1~5 into one variable\n",
    "6. Add the resultant string into a list.\n",
    "7. The resultant list is a list of all train + test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Action  Adventure  Fantasy  Science Fiction  ...\n",
       "1                      Adventure  Fantasy  Action        \n",
       "2                        Action  Adventure  Crime        \n",
       "3                    Action  Crime  Drama  Thriller      \n",
       "4              Action  Adventure  Science Fiction        \n",
       "                              ...                        \n",
       "4798                      Action  Crime  Thriller        \n",
       "4799                            Comedy  Romance          \n",
       "4800               Comedy  Drama  Romance  TV Movie      \n",
       "4801                                                     \n",
       "4802                              Documentary            \n",
       "Length: 4803, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_genres = df['genres'].str[1:-1]\n",
    "split_genres = split_genres.str.split(', ', expand=True)\n",
    "\n",
    "# Extract genre using regex\n",
    "# Iterating over each row is slow and inefficient. Use applymap or apply instead.\n",
    "def extract_name(cell):\n",
    "    if pd.isnull(cell):\n",
    "        return ''\n",
    "    match = re.search(r'\"name\":\\s*\"([^\"]*)\"', cell) # cannot search through None\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Apply extract_genre function\n",
    "genre_extracted_df = split_genres.applymap(extract_name)\n",
    "genre_extracted_df = genre_extracted_df.fillna('')\n",
    "\n",
    "genre_extracted_df = genre_extracted_df.agg(' '. join, axis=1)\n",
    "\n",
    "genre_extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        culture clash  future  space war  space colon...\n",
       "1        ocean  drug abuse  exotic island  east india ...\n",
       "2        spy  based on novel  secret agent  sequel  mi...\n",
       "3        dc comics  crime fighter  terrorist  secret i...\n",
       "4        based on novel  mars  medallion  space travel...\n",
       "                              ...                        \n",
       "4798     united states\\u2013mexico barrier  legs  arms...\n",
       "4799                                                  ...\n",
       "4800     date  love at first sight  narration  investi...\n",
       "4801                                                  ...\n",
       "4802     obsession  camcorder  crush  dream girl      ...\n",
       "Length: 4803, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_keywords = df['keywords'].str[1:-1]\n",
    "split_keywords = split_keywords.str.split(', ', expand=True)\n",
    "\n",
    "keywords_extracted_df = split_keywords.applymap(extract_name)\n",
    "keywords_extracted_df = keywords_extracted_df.fillna('')\n",
    "\n",
    "keywords_extracted_df = keywords_extracted_df.agg(' '. join, axis=1)\n",
    "\n",
    "keywords_extracted_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_df = df['overview']\n",
    "tagline_df = df['tagline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Action  Adventure  Fantasy  Science Fiction  ...\n",
       "1        Adventure  Fantasy  Action          ocean  dr...\n",
       "2        Action  Adventure  Crime          spy  based ...\n",
       "3        Action  Crime  Drama  Thriller        dc comi...\n",
       "4        Action  Adventure  Science Fiction          b...\n",
       "                              ...                        \n",
       "4798     Action  Crime  Thriller          united state...\n",
       "4799     Comedy  Romance                              ...\n",
       "4800     Comedy  Drama  Romance  TV Movie        date ...\n",
       "4801                                                  ...\n",
       "4802     Documentary              obsession  camcorder...\n",
       "Length: 4803, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = pd.concat([genre_extracted_df, keywords_extracted_df, overview_df, tagline_df], axis=1)\n",
    "total_df = total_df.fillna('')\n",
    "\n",
    "total_df = total_df.agg(' '. join, axis=1)\n",
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using TF-IDF\n",
    "\n",
    "No need to split the data into train and test set, because there is no testing to be done. \n",
    "Rather, we are calculating the vector distance between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = total_df \n",
    "\n",
    "tfidf = TfidfVectorizer() # instantiate TfidfVectorizer class\n",
    "                          # try out other 변수 like stopwords\n",
    "tfidf_matrix = tfidf.fit_transform(train_texts) # fit vectorizer onto data, transform into vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get query, calculate the closest 5 vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the TF-IDF of a query movie, compute similarity between query and other vectors\n",
    "query_movie = input('Which movie do you wish to watch: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your query, I would recommended: Jab Tak Hai Jaan, Free Birds, The Others, Flight, Spy Game\n"
     ]
    }
   ],
   "source": [
    "# take query_movie find corresponding vector in df\n",
    "query_movie_index = df[df['original_title'] == query_movie].index[0]\n",
    "query_vector = tfidf_matrix[query_movie_index]\n",
    "\n",
    "# calculate cosine distance between corresponding vector and all other vectors using df\n",
    "cosine_similarity_list = []\n",
    "for i in range(1, 4803):\n",
    "    query_vector_1D = query_vector.toarray().flatten()\n",
    "    tfidf_matrix_1D = tfidf_matrix[i].toarray().flatten()\n",
    "    cosine_similarity = 1 - spatial.distance.cosine(query_vector_1D, tfidf_matrix_1D)\n",
    "    cosine_similarity_list.append(cosine_similarity)\n",
    "\n",
    "# rank the distance, select the 5 closest vectors\n",
    "index = np.argpartition(cosine_similarity_list, -5)[-5:]\n",
    "\n",
    "# return the movies that correspond with those 5 closest vectors using df\n",
    "print('Based on your query, I would recommended: {0}, {1}, {2}, {3}, {4}'.format(\n",
    "    df.loc[index[0],'original_title'], \n",
    "    df.loc[index[1],'original_title'], \n",
    "    df.loc[index[2],'original_title'], \n",
    "    df.loc[index[3],'original_title'], \n",
    "    df.loc[index[4],'original_title']\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
